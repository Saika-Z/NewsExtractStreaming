该项目涉及服务的启动顺序:
zookeeper
hadoop(yarn)
hbase
kafka
kafka-manager
redis-cluster
python_crawler
redis
报表
es(es-ik)
es-head
es-sql
mysql
spark-streaming
crontab python_xpath

mysql报表结构：
CREATE TABLE `report_stream_extract` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT 'id',
  `host` varchar(100) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'host地址',
  `scan` bigint(20) NOT NULL DEFAULT '0' COMMENT '扫描数量',
  `filtered` bigint(20) NOT NULL DEFAULT '0' COMMENT '过滤数量',
  `extract` bigint(20) NOT NULL DEFAULT '0' COMMENT '抽取成功数量',
  `empty_context` bigint(20) NOT NULL DEFAULT '0' COMMENT '空内容数量',
  `no_match_xpath` bigint(20) NOT NULL DEFAULT '0' COMMENT '没有匹配到xpath数量',
  `scan_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '第一次扫描时间',
  `scan_day` int(11) NOT NULL DEFAULT '0' COMMENT '第一次扫描的时间天维度',
  `scan_hour` int(11) NOT NULL DEFAULT '0' COMMENT '第一次扫描的时间小时纬度',
  `hour_md5` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT '按扫描时间的小时生成md5值，用于按小时合并数据',
  PRIMARY KEY (`host`,`hour_md5`) USING BTREE,
  KEY `id` (`id`),
  KEY `hour_md5` (`hour_md5`),
  KEY `host` (`host`),
  KEY `host_hour_md5` (`host`,`hour_md5`) USING BTREE,
  KEY `scan_day` (`scan_day`),
  KEY `scan_hour` (`scan_hour`),
  KEY `scan_day_hour` (`scan_day`,`scan_hour`),
  KEY `scan_day_host` (`scan_day`,`host`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;


MySQL人工配置表
CREATE TABLE `c20_zhaowq_stream_extract_xpath_rule`(
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `host` varchar(200) NOT NULL COMMENT '网站host',
  `xpath` varchar(2000) NOT NULL COMMENT 'xpath规则',
  `type` tinyint(2) NOT NULL COMMENT '类型:0正规则，1反规则',
  `status` tinyint(2) NOT NULL DEFAULT '1' COMMENT '状态 0 启用 1 关闭',
  `create_times` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '记录创建时间',
  PRIMARY KEY (`id`),
  KEY `host` (`host`),
  KEY `host_status` (`hos t`,`status`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;


hbase 表
-- i: 网页元数据 url、domain、host
-- c: 抽取的正文
-- h: 网页HTML源码， 由于HTML源码只是作为错误检查时用，不经常查询，所以没有设置读缓存
create 'context_extract',
{NAME => 'i', VERSIONS => 1, BLOCKCACHE => true,COMPRESSION => 'SNAPPY'},
{NAME => 'c', VERSIONS => 1, BLOCKCACHE => true,COMPRESSION => 'SNAPPY'},
{NAME => 'h', VERSIONS => 1,COMPRESSION => 'SNAPPY'}



//创建5个自定义累加器, 用来记录每个批次中每个host的抽取情况
    val hostScanAccumulator = new MapAccumulator
    val hostFilteredAccumulator = new MapAccumulator
    val hostExtractAccumulator = new MapAccumulator
    val hostEmptyAccumulator = new MapAccumulator
    val hostNoMatchAccumulator = new MapAccumulator
    ssc.sparkContext.register(hostScanAccumulator)
    ssc.sparkContext.register(hostFilteredAccumulator)
    ssc.sparkContext.register(hostExtractAccumulator)
    ssc.sparkContext.register(hostEmptyAccumulator)
    ssc.sparkContext.register(hostNoMatchAccumulator)
 // 累加
    hostScanAccumulator.add(host -> 1L)